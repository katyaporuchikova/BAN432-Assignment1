---
title: "Group Assignment 1"
author: "Group"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r data}
setwd('C:/Users/steef/Documents/NHH/BAN432 Applied Textual Data Analysis for Business and Finance/Assignment Group 1/BAN432-Assignment1')
load('firm_dataset.Rdata')
rm(section.7.mda)
```

```{r libraries-and-functions}
library(tm)
library(tidytext)
library(stringr)
library(tibble)
library(dplyr)
```


```{r data-cleaning}
# Clean the data: remove punctuation, remove numbers, remove stopwords, make lower case, only consider
# terms with 3 to 20 letters, and delete excess white spaces.

section.1.business %>%
  # Remove the \" 
  gsub('\\\"','',.) %>% 
  # Transform to lowercase
  tolower() %>% 
  removePunctuation() %>% 
  removeNumbers() %>% 
  # Remove stopwords
  removeWords(stopwords()) %>% 
  # Remove extra spaces
  gsub(' +',' ',.) %>% 
  # Remove words with 1 to 3 characters
  gsub('\\b\\w{1,3}\\s','',.) %>% 
  # Remove words with 21 or more characters
  gsub('\\b\\w{21,}\\s','',.) -> sec.1.clean 

```

```{r tokensize-bigrams}
# Transform the data into bigrams. One easy implementation using the tokenizers package:

sec.1.clean %>%
  as_tibble()  %>%
  mutate(rown = c(1:500)) %>% # create number for future merging with raw.data
  unnest_ngrams(input = 'value', 
                output = 'bigram', 
                n = 2,
                ngram_delim = '_') -> sec.1.bigrams

```

```{r make-dtm, message = FALSE}
# Make a document term matrix only including tokens that appear in more than 5 
# but less than 100 documents.

sec.1.bigrams %>%
  group_by(rown) %>%
  select(bigram) %>%
  summarise(text = paste(bigram, collapse = ' '), .groups = 'drop') %>%
  select(text) %>%
  # To not lose the document numbers
  as.vector() %>%
  .[[1]] %>%
  VectorSource(.) %>%
  Corpus(.) %>%
  DocumentTermMatrix(control = list(bound = list(global = c(5, 100)))) -> dtm

```

```{r compute-log-lik}
# For each token (bi-grams) in the corpora compute the Log-Likelihood of being in the subset of oil firms
# and the remaining. Display the top tokens. Are these tokens associated with the oil sector? For the
# remainder of the analysis, only use the 500 tokens with the highest Log-Likelihood. Log-Likelihood was
# mentioned in one of the guest lectures and you calculate it this way:
dtm[raw.data$industry.fama.french.49 == '30 Oil',] 
dtm[raw.data$industry.fama.french.49 != '30 Oil',]


calculate.ll <- function(a, b, c, d) {
  e1 <- c * (a + b) / (c + d)
  e2 <- d * (a + b) / (c + d)
  ll <- 2 * ((a * log(a / e1)) + (b * log(b / e2)))
  return(ll)
}

# a = freq. of a word in the oil corpus
a = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# b = freq. of a word in the non-oil corpus
b = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# c = sum of all words in the oil corpus
c = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  sum()

# d = sum of all words in the non-oil corpus
d = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  sum()

cbind(as.data.frame(a),b) %>%
  filter((a != 0) & # If a = 0 log is not defined
           (b != 0)) %>% # If b = 0 log is not defined
  mutate(e.1 = c * (a + b) / (c + d), 
         e.2 = d * (a + b) / (c + d)) %>%
  mutate(ll = 2 * ((a * log(a / e.1)) + (b * log(b / e.2)))) %>%
  select(ll) %>%
  arrange(desc(ll))

```



