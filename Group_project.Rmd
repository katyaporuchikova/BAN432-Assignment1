---
title: "Group Assignment 1"
author: "Group"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r data}
setwd('C:/Users/steef/Documents/NHH/BAN432 Applied Textual Data Analysis for Business and Finance/Assignment Group 1/BAN432-Assignment1')
load('firm_dataset.Rdata')
rm(section.7.mda)
```

```{r libraries-and-functions}
library(tm)
library(tidytext)
library(stringr)
library(tibble)
library(dplyr)
library(ggplot2)
library(geomtextpath)
```

```{r data-cleaning}
# Clean the data: remove punctuation, remove numbers, remove stopwords, make lower case, only consider
# terms with 3 to 20 letters, and delete excess white spaces.

section.1.business %>%
  # Remove the \" 
  gsub('\\\"','',.) %>% 
  # Transform to lowercase
  tolower() %>% 
  removePunctuation() %>% 
  removeNumbers() %>% 
  # Remove stopwords
  removeWords(stopwords()) %>% 
  # Remove extra spaces
  gsub(' +',' ',.) %>% 
  # Remove words with 1 to 3 characters
  gsub('\\b\\w{1,3}\\s','',.) %>% 
  # Remove words with 21 or more characters
  gsub('\\b\\w{21,}\\s','',.) -> sec.1.clean 

```

```{r tokensize-bigrams}
# Transform the data into bigrams. One easy implementation using the tokenizers package:

sec.1.clean %>%
  as_tibble()  %>%
  mutate(rown = c(1:500)) %>% # create number for future merging with raw.data
  unnest_ngrams(input = 'value', 
                output = 'bigram', 
                n = 2,
                ngram_delim = '_') -> sec.1.bigrams

```

```{r make-dtm, message = FALSE}
# Make a document term matrix only including tokens that appear in more than 5 
# but less than 100 documents.

sec.1.bigrams %>%
  group_by(rown) %>%
  select(bigram) %>%
  summarise(text = paste(bigram, collapse = ' '), .groups = 'drop') %>%
  select(text) %>%
  # To not lose the document numbers
  as.vector() %>%
  .[[1]] %>%
  VectorSource(.) %>%
  Corpus(.) %>%
  DocumentTermMatrix(control = list(bound = list(global = c(5, 100)))) -> dtm

```

```{r compute-log-lik}
# For each token (bi-grams) in the corpora compute the Log-Likelihood of being in the subset of oil firms
# and the remaining. Display the top tokens. Are these tokens associated with the oil sector? For the
# remainder of the analysis, only use the 500 tokens with the highest Log-Likelihood. Log-Likelihood was
# mentioned in one of the guest lectures and you calculate it this way:
calculate.ll <- function(a, b, c, d) {
  e1 <- c * (a + b) / (c + d)
  e2 <- d * (a + b) / (c + d)
  ll <- 2 * ((a * log(a / e1)) + (b * log(b / e2)))
  return(ll)
}

# a = freq. of a word in the oil corpus
a = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# b = freq. of a word in the non-oil corpus
b = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# c = sum of all words in the oil corpus
c = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  sum()

# d = sum of all words in the non-oil corpus
d = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  sum()

cbind(as.data.frame(a),b) %>%
  filter((a != 0) & # If a = 0 log is not defined
           (b != 0)) %>% # If b = 0 log is not defined
  mutate(e.1 = c * (a + b) / (c + d), 
         e.2 = d * (a + b) / (c + d)) %>%
  mutate(ll = 2 * ((a * log(a / e.1)) + (b * log(b / e.2)))) %>%
  select(ll) %>%
  arrange(desc(ll)) %>%
  head(500) -> top.500.ll

print(top_n(top.500.ll, n = 10))

imp.bigrams <- rownames(top.500.ll) 

# Use the 500 tokens with the highest Log-Likelihood
dtm <- dtm[,imp.bigrams]
```

```{r cos-sim-peers}

CosineSimilarity <- function(A, B) {
  sum(A * B) / sqrt(sum(A ^ 2) * sum(B ^ 2))
}

d <- data.frame(matrix(ncol = 18, nrow = 482))

filter(raw.data, raw.data$industry.fama.french.49 == "30 Oil") -> Oil
filter(raw.data, raw.data$industry.fama.french.49 != "30 Oil") -> notOil

for (i in 1:18) {
  for (j in 1:482) {
    d[[j, i]] <- CosineSimilarity(dtm[raw.data$cik == Oil$cik[i], ],
                                  dtm[raw.data$cik == notOil$cik[j], ])
  }
}

# Create a list for peer firms
closest <- list()

# Select top 5
for (i in 1:18) {
  which(d[, i] %in% paste(sort(d[, i], decreasing = TRUE)[1:5])) %>%
    append(closest, .) -> closest
}

# Indexes to doc numbers
map_dbl(closest, 
        ~ which(raw.data$industry.fama.french.49 != '30 Oil')[.x]) -> 
  closest.ind
```

```{r avg-return}
# Add weights 
sort(closest.ind) %>%
  as_tibble() %>%
  group_by(value) %>%
  summarize(weight = n()) %>%
  mutate(weight = weight/length(closest)) -> portfolio.composition

# Equally weighted portfolio
raw.data %>%
  filter(industry.fama.french.49 == '30 Oil') %>%
  select(return.monthly.NY.m01 : return.monthly.NY.m12) %>%
  colMeans() %>%
  as_tibble() %>%
  rename(Oil.return = value) -> Oil.ret

# Weighted peer portfolio return
raw.data[portfolio.composition$value,] %>%
  select(return.monthly.NY.m01 : return.monthly.NY.m12) %>%
  bind_cols(portfolio.composition) %>%
  select(-value) %>%
  pivot_longer(return.monthly.NY.m01 : return.monthly.NY.m12,
               names_to = 'month', values_to = 'return') %>%
  group_by(month) %>%
  summarize(portf.ret = sum(weight * return)) %>%
  as_tibble() %>%
  bind_cols(Oil.ret) -> replicating.portf

# Plot replicating portfolio
replicating.portf %>%
  pivot_longer(portf.ret:Oil.return, 
               names_to = 'Portfolio', values_to = 'Return') %>%
  mutate(month = gsub('return.monthly.NY.m', '', month) %>%
           as.numeric()) %>%
  ggplot(aes(x = month, y = Return, color = Portfolio)) +
  geom_line(size = 1) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  scale_x_continuous(breaks = c(1:12), 
                     labels = month.abb) +
  labs(x = 'Month') +
  scale_color_manual(values = c('red', 'blue'))

```

Evaluate the performance of your algorithm. Compute the Root Mean Squared Error (just google the definition) between both portfolios.

```{r evaluate-performance}
replicating.portf %>%
  mutate(RMSE = (Oil.return - portf.ret)^2) %>%
  select(RMSE) %>%
  colSums()/12 -> RMSE.bigrams

RMSE.bigrams
```

Evaluate whether using uni-grams (versus bi-grams) performs better/worse? For the remainder of steps,
use exactly the same approach. An easy way to implement this analysis is by recycling your current
code skipping the step in which you transform the corpus into bigrams.

# Uni-grams

```{r uni-grams}
sec.1.clean %>%
  VectorSource(.) %>%
  Corpus(.) %>%
  DocumentTermMatrix(control = list(bound = list(global = c(5, 100)))) -> dtm

# a = freq. of a word in the oil corpus
a = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# b = freq. of a word in the non-oil corpus
b = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  colSums()

# c = sum of all words in the oil corpus
c = dtm[raw.data$industry.fama.french.49 == '30 Oil', ] %>%
  as.matrix() %>%
  sum()

# d = sum of all words in the non-oil corpus
d = dtm[raw.data$industry.fama.french.49 != '30 Oil', ] %>%
  as.matrix() %>%
  sum()

cbind(as.data.frame(a),b) %>%
  filter((a != 0) & # If a = 0 log is not defined
           (b != 0)) %>% # If b = 0 log is not defined
  mutate(e.1 = c * (a + b) / (c + d), 
         e.2 = d * (a + b) / (c + d)) %>%
  mutate(ll = 2 * ((a * log(a / e.1)) + (b * log(b / e.2)))) %>%
  select(ll) %>%
  arrange(desc(ll)) %>%
  head(500) -> top.500.ll

print(top_n(top.500.ll, n = 10))

imp.bigrams <- rownames(top.500.ll) 

# Use the 500 tokens with the highest Log-Likelihood
dtm <- dtm[,imp.bigrams]

CosineSimilarity <- function(A, B) {
  sum(A * B) / sqrt(sum(A ^ 2) * sum(B ^ 2))
}

d <- data.frame(matrix(ncol = 18, nrow = 482))

filter(raw.data, raw.data$industry.fama.french.49 == "30 Oil") -> Oil
filter(raw.data, raw.data$industry.fama.french.49 != "30 Oil") -> notOil

for (i in 1:18) {
  for (j in 1:482) {
    d[[j, i]] <- CosineSimilarity(dtm[raw.data$cik == Oil$cik[i], ],
                                  dtm[raw.data$cik == notOil$cik[j], ])
  }
}

# Create a list for peer firms
closest <- list()

# Select top 5
for (i in 1:18) {
  which(d[, i] %in% paste(sort(d[, i], decreasing = TRUE)[1:5])) %>%
    append(closest, .) -> closest
}

# Indexes to doc numbers
map_dbl(closest, 
        ~ which(raw.data$industry.fama.french.49 != '30 Oil')[.x]) -> 
  closest.ind

# Add weights 
sort(closest.ind) %>%
  as_tibble() %>%
  group_by(value) %>%
  summarize(weight = n()) %>%
  mutate(weight = weight/length(closest)) -> portfolio.composition

# Equally weighted portfolio
raw.data %>%
  filter(industry.fama.french.49 == '30 Oil') %>%
  select(return.monthly.NY.m01 : return.monthly.NY.m12) %>%
  colMeans() %>%
  as_tibble() %>%
  rename(Oil.return = value) -> Oil.ret

# Weighted peer portfolio return
raw.data[portfolio.composition$value,] %>%
  select(return.monthly.NY.m01 : return.monthly.NY.m12) %>%
  bind_cols(portfolio.composition) %>%
  select(-value) %>%
  pivot_longer(return.monthly.NY.m01 : return.monthly.NY.m12,
               names_to = 'month', values_to = 'return') %>%
  group_by(month) %>%
  summarize(portf.ret = sum(weight * return)) %>%
  as_tibble() %>%
  bind_cols(Oil.ret) -> replicating.portf

# Plot replicating portfolio
replicating.portf %>%
  pivot_longer(portf.ret:Oil.return, 
               names_to = 'Portfolio', values_to = 'Return') %>%
  mutate(month = gsub('return.monthly.NY.m', '', month) %>%
           as.numeric()) %>%
  ggplot(aes(x = month, y = Return, color = Portfolio)) +
  geom_line(size = 1) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  scale_x_continuous(breaks = c(1:12), 
                     labels = month.abb) +
  labs(x = 'Month') +
  scale_color_manual(values = c('red', 'blue'))

replicating.portf %>%
  mutate(RMSE = (Oil.return - portf.ret)^2) %>%
  select(RMSE) %>%
  colSums()/12 -> RMSE.uni.grams

RMSE.uni.grams
```

â€¢ [Optional] Does this approach of constructing tracking portfolios work? Knowing the value of the RMSE
is not necessarily enough to assess performance against alternative portfolio selections. Construct 10.000
random portfolios of similar number of firms and compute the corresponding RMSE. Display it as a
histogram. Does the text approach do a better/worse job?

```{r optional}
size <- 10000

rm(replicating.portf.rand)

replicating.portf.rand <- data.frame(matrix(ncol = size, nrow = 12))

for (i in 1:size){
  # Sample 38 stocks
  sample(which(raw.data$industry.fama.french.49 != "30 Oil"),
         nrow(portfolio.composition), replace = TRUE) %>% # Allow twice

  as_tibble() %>%
  group_by(value) %>%
  summarize(weight = n()) %>%
  mutate(weight = weight/length(closest)) -> portfolio.composition

  raw.data[portfolio.composition$value,] %>%
  select(return.monthly.NY.m01 : return.monthly.NY.m12) %>%
  bind_cols(portfolio.composition) %>%
  select(-value) %>%
  pivot_longer(return.monthly.NY.m01 : return.monthly.NY.m12,
               names_to = 'month', values_to = 'return') %>%
  group_by(month) %>%
  summarize(portf.ret = sum(weight * return)) %>%
  as_tibble() %>%
    select(portf.ret) -> replicating.portf.rand[,i]
}

(replicating.portf.rand - replicating.portf$Oil.return)^2 %>%
  colSums() / 12 -> RMSE.rand

RMSE.rand %>%
  as_tibble() %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 30, fill = 'blue', alpha = 0.5) +
  geom_textvline(label = 'Bigrams', 
                 xintercept = RMSE.bigrams, vjust = 1.3,
                 color = 'black') +
  geom_textvline(label = 'Uni-grams', 
                 xintercept = RMSE.uni.grams, vjust = 1.3,
                 color = 'black') +
  geom_textvline(label = 'Minimum RMSE random portfolio', 
                 xintercept = min(RMSE.rand), vjust = 1.3,
                 color = 'blue') +
  theme_minimal() +
  theme(panel.grid = element_blank()) 

```





